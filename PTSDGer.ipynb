{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "nope\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import csv\n",
    "\n",
    "tree = ET.parse(r\"C:\\Users\\Billy.Peacock\\Desktop\\PythonWebScraping\\ICTRP-Results.xml\")\n",
    "root = tree.getroot()\n",
    "\n",
    "trials = root.findall('Trial')\n",
    "\n",
    "triallist =[]\n",
    "for trialid in root.iter('TrialID'):\n",
    "    triallist.append(trialid.text)\n",
    "\n",
    "\n",
    "with open('totallist.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for row in csv_reader:\n",
    "        totallist = row\n",
    "        \n",
    "intersection_set = set.intersection(set(triallist), set(totallist))\n",
    "intersection_list = list(intersection_set)\n",
    "\n",
    "sorted_old = sorted(totallist)\n",
    "sorted_new = sorted(intersection_list)\n",
    "print(len(sorted_old)- len(sorted_new))\n",
    "if sorted_old == sorted_new:\n",
    "    print(\"gottem\")\n",
    "else:\n",
    "    print(\"nope\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'strip_cdata' is an invalid keyword argument for XMLParser()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-990c75e1190e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXMLParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrip_cdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mET\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\Billy.Peacock\\Desktop\\PythonWebScraping\\DRKS00003704.xml\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'strip_cdata' is an invalid keyword argument for XMLParser()"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "parser = ET.XMLParser(strip_cdata=False)\n",
    "tree = ET.parse(r\"C:\\Users\\Billy.Peacock\\Desktop\\PythonWebScraping\\DRKS00003704.xml\", parser)\n",
    "root = tree.getroot()\n",
    "print(root.tag)\n",
    "\n",
    "for pholder in root.iter('drksId'):\n",
    "    print(pholder)\n",
    "\n",
    "helpme = root.find('firstDrksPublishDate')\n",
    "print(helpme)\n",
    "\n",
    "print(\"working\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import csv\n",
    "\n",
    "newlist = []\n",
    "with open('trials.csv', encoding = \"utf-8\") as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        newlist.append(row)\n",
    "\n",
    "\n",
    "with open(\"output.csv\", \"w\", newline=\"\", encoding = \"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(newlist)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRKS00019876\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "parsed = etree.parse(r\"C:\\Users\\Billy.Peacock\\Desktop\\PythonWebScraping\\DRKS00019876.xml\")\n",
    "string_parsed = etree.tostring(parsed)\n",
    "\n",
    "root = parsed.getroot()\n",
    "ns = root.tag\n",
    "ns = ns[ns.find(\"{\")+1:ns.find(\"}\")]\n",
    "term = 'drksId'\n",
    "findterm = '{'+ns+'}' + term\n",
    "print(root.find(findterm).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRKS00003275.xml\n",
      "DRKS00008745.xml\n",
      "DRKS00008927.xml\n",
      "DRKS00008939.xml\n",
      "DRKS00009007.xml\n",
      "DRKS00009399.xml\n",
      "DRKS00010118.xml\n",
      "DRKS00010146.xml\n",
      "DRKS00010245.xml\n",
      "DRKS00010551.xml\n",
      "DRKS00010676.xml\n",
      "DRKS00010827.xml\n",
      "DRKS00011158.xml\n",
      "DRKS00011555.xml\n",
      "DRKS00012268.xml\n",
      "DRKS00012589.xml\n",
      "DRKS00012839.xml\n",
      "DRKS00013782.xml\n",
      "DRKS00014280.xml\n",
      "DRKS00014762.xml\n",
      "DRKS00014848.xml\n",
      "DRKS00014913.xml\n",
      "DRKS00015182.xml\n",
      "DRKS00015303.xml\n",
      "DRKS00015325.xml\n",
      "DRKS00015817.xml\n",
      "DRKS00015875.xml\n",
      "DRKS00016154.xml\n",
      "DRKS00016307.xml\n",
      "DRKS00016538.xml\n",
      "DRKS00016607.xml\n",
      "DRKS00016931.xml\n",
      "DRKS00017397.xml\n",
      "DRKS00017453.xml\n",
      "DRKS00017664.xml\n",
      "DRKS00017749.xml\n",
      "DRKS00017843.xml\n",
      "DRKS00018818.xml\n",
      "DRKS00019851.xml\n",
      "Finished in 0.5540680885314941 Seconds\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import xlsxwriter as xlsx\n",
    "import fnmatch\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "import openpyxl\n",
    "import csv\n",
    "\n",
    "start = time.time()\n",
    "files = os.listdir(r\"C:\\Users\\Billy.Peacock\\Desktop\\PythonWebScraping\\PTSDGerData\")\n",
    "\n",
    "workbook = xlsx.Workbook('PTSDGerData1.xlsx')\n",
    "\n",
    "worksheetnames = workbook.add_worksheet(\"Names\")\n",
    "worksheetnonames = workbook.add_worksheet(\"NoNames\")\n",
    "worksheetcompare = workbook.add_worksheet(\"Compare\")\n",
    "\n",
    "worksheetnames.write(0, 0, \"First_Name\")\n",
    "worksheetnames.write(0, 1, \"Last_name\")\n",
    "worksheetnames.write(0, 2, \"Official_Name\")\n",
    "worksheetnames.write(0, 3, \"Official_Role\")\n",
    "worksheetnames.write(0, 4, \"Official_Affil\")\n",
    "worksheetnames.write(0, 5, \"ID\")\n",
    "worksheetnames.write(0, 6, \"Official_Title\")\n",
    "worksheetnames.write(0, 7, \"Condition\")\n",
    "worksheetnames.write(0, 8, \"Intervention_Name\")\n",
    "worksheetnames.write(0, 9, \"Agency\")\n",
    "worksheetnames.write(0, 10, \"Phase\")\n",
    "worksheetnames.write(0, 11, \"Start_Date\")\n",
    "worksheetnames.write(0, 12, \"Completion_Date\")\n",
    "worksheetnames.write(0, 13, \"Last_Updated\")\n",
    "worksheetnames.write(0, 14, \"Location\")\n",
    "worksheetnames.write(0, 15, \"URL\")\n",
    "worksheetnames.write(0, 16, \"File_Ref\")\n",
    "\n",
    "\n",
    "worksheetnonames.write(0, 0, \"ID\")\n",
    "worksheetnonames.write(0, 1, \"Official_Title\")\n",
    "worksheetnonames.write(0, 2, \"Condition\")\n",
    "worksheetnonames.write(0, 3, \"Intervention_Name\")\n",
    "worksheetnonames.write(0, 4, \"Agency\")\n",
    "worksheetnonames.write(0, 5, \"Phase\")\n",
    "worksheetnonames.write(0, 6, \"Start_Date\")\n",
    "worksheetnonames.write(0, 7, \"Completion_Date\")\n",
    "worksheetnonames.write(0, 8, \"Last_Updated\")\n",
    "worksheetnonames.write(0, 9, \"Location\")\n",
    "worksheetnonames.write(0, 10, \"URL\")\n",
    "worksheetnonames.write(0, 11, \"File_Ref\")\n",
    "\n",
    "worksheetcompare.write(0,0, \"NZ_Trials\")\n",
    "worksheetcompare.write(0,1, \"Old_Missing\")\n",
    "worksheetcompare.write(0,2, \"Sorted_New\")\n",
    "worksheetcompare.write(0,3, \"Sorted_Old\")\n",
    "\n",
    "\n",
    "row = 1\n",
    "col = 0\n",
    "row1 = 1\n",
    "col1 = 0\n",
    "\n",
    "Names_ID = []\n",
    "NoNames_ID = []\n",
    "\n",
    "for file in files:\n",
    "    if fnmatch.fnmatch(file, '*.XML'):\n",
    "        working_file = os.path.join(r\"C:\\Users\\Billy.Peacock\\Desktop\\PythonWebScraping\\PTSDGerData\", file)\n",
    "        print(file)\n",
    "        tree = ET.parse(working_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        all_params = []\n",
    "        new_all_params = []\n",
    "        conditionslist = []\n",
    "        \n",
    "        ns = root.tag\n",
    "        ns = ns[ns.find(\"{\")+1:ns.find(\"}\")]\n",
    "        ns = '{'+ns+'}'\n",
    "\n",
    "        addresses = root.find(ns + 'addresses')\n",
    "        for address in addresses.findall(ns + 'address'):\n",
    "            roledict = address.attrib\n",
    "            role = roledict['type']\n",
    "            \n",
    "            firstnametest = address.find(ns + 'firstname')\n",
    "            if firstnametest is not None:\n",
    "                firstname = firstnametest.text\n",
    "                all_params.append(firstname)\n",
    "            else:\n",
    "                firstname = ''\n",
    "                all_params.append(' ')\n",
    "                \n",
    "            lastnametest = address.find(ns + 'lastname')\n",
    "            if lastnametest is not None:\n",
    "                lastname = lastnametest.text\n",
    "                all_params.append(lastname)\n",
    "                fullname = firstname + ' ' + lastname\n",
    "                all_params.append(fullname)\n",
    "            else:\n",
    "                all_params.extend((' ', ' '))\n",
    "                \n",
    "            \n",
    "            \n",
    "            all_params.append(role)\n",
    "            \n",
    "            affiltest = address.find(ns + 'affiliation')\n",
    "            if affiltest is not None:\n",
    "                affil = affiltest.text\n",
    "                all_params.append(affil)\n",
    "            else:\n",
    "                all_params.append(' ')\n",
    "            \n",
    "            ID = root.find(ns + 'drksId').text\n",
    "            \n",
    "            all_params.append(ID)\n",
    "            \n",
    "            description = root.find(ns +'description')\n",
    "            titleholder = description.find(ns + 'title')\n",
    "            contents = titleholder.find(ns + 'contents')\n",
    "            titles= contents.findall(ns + 'localizedContent')\n",
    "            \n",
    "            for title in titles:\n",
    "                if title.attrib['locale'] == 'en':\n",
    "                    all_params.append(title.text)\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            indications = root.find(ns + 'indications')\n",
    "            for indication in indications.findall(ns + 'indication'):\n",
    "                value = indication.find(ns + 'value')\n",
    "                contents = value.find(ns + 'contents')\n",
    "                if contents is not None:\n",
    "                    for loccont in contents.findall(ns + 'localizedContent'):\n",
    "                        if loccont.attrib['locale'] == 'en':\n",
    "                            conditionslist.append(loccont.text)\n",
    "                        else:\n",
    "                            pass\n",
    "                else:\n",
    "                    pass\n",
    "                    \n",
    "                conditions = ', '.join(conditionslist)\n",
    "            conditionslist = []\n",
    "            all_params.append(conditions)\n",
    "            \n",
    "            interventions = ' '\n",
    "            all_params.append(interventions)\n",
    "            \n",
    "            materialsupports = root.find(ns + 'materialSupports')\n",
    "            materialsupport = materialsupports.find(ns + 'materialSupport')\n",
    "            address = materialsupport.find(ns + 'address')\n",
    "            agencytest = address.find(ns + 'affiliation')\n",
    "            if agencytest is not None:\n",
    "                agency = agencytest.text\n",
    "                all_params.append(agency)\n",
    "            else:\n",
    "                all_params.append(' ')\n",
    "            \n",
    "            study = root.find(ns + 'study')\n",
    "            phase = study.find(ns + 'phase')\n",
    "            phasecont = phase.find(ns + 'contents')\n",
    "            if phasecont is not None:\n",
    "                phasenames = phasecont.findall(ns + 'localizedContent')\n",
    "                for phasename in phasenames:\n",
    "                    if phasename.attrib['locale'] == 'en':\n",
    "                        all_params.append(phasename.text)\n",
    "                    else:\n",
    "                        pass\n",
    "            else:\n",
    "                all_params.append(' ')\n",
    "            \n",
    "            recruitment = root.find(ns + 'recruitement')\n",
    "            if recruitment is not None:\n",
    "                startdate = recruitment.find(ns + 'schedule')\n",
    "                startdate = startdate.text\n",
    "                startdate = startdate[0:10]\n",
    "                all_params.append(startdate)\n",
    "            else:\n",
    "                all_params.append(' ')           \n",
    "            \n",
    "            enddate = lastupdated = location = url = ' '\n",
    "            all_params.extend((enddate, lastupdated, location, url, file))\n",
    "              \n",
    "            for i in all_params:\n",
    "                worksheetnames.write(row, col, i)\n",
    "                col += 1\n",
    "            row+=1\n",
    "            col = 0\n",
    "                \n",
    "            all_params = []\n",
    "                \n",
    "worksheetnames.set_column(0,20,20)\n",
    "worksheetnonames.set_column(0,20,20)\n",
    "worksheetcompare.set_column(0,20,20)\n",
    "\n",
    "workbook.close()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Finished in\", end-start, \"Seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
